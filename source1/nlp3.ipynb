{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXEmfRK89vjgqE9FoysLYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultimatecrack/NLP-Master/blob/master/source1/nlp3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIdy3RoD_AkW",
        "colab_type": "text"
      },
      "source": [
        "#Skip-gram\n",
        "Understand the difference between skip-gram and CBOW embedding models.\n",
        "\n",
        "Chapter Goals:\n",
        "  1.Learn about the types of models used to create embeddings\n",
        "  2.Understand the difference between the skip-gram and CBOW models\n",
        "\n",
        "**A. Embedding models**\n",
        "\n",
        "In the previous chapter, we discussed how embedding vectors are based on target words and context windows. Specifically, we use these target words and context windows to create training pairs. Depending on the type of embedding model we use, the pairs will either be target-context pairs or context-target pairs.\n",
        "\n",
        "**Skip-gram**\n",
        "\n",
        "The skip-gram model uses target-context training pairs. Each pair has a target word as the first element and a context word as the second element. This means that every context window will produce multiple training pairs. For example, if the context window is \"paul likes ***singing*** in french\", (where ***singing*** is the target word), the training pairs would be\n",
        "\n",
        "(singing, paul), (singing, likes)\n",
        "(singing, in), (singing, french)\n",
        "\n",
        "When training an embedding model, we convert the first element in each pair (for the skip-gram model, this is the target word) into its corresponding embedding vector. This will be discussed in greater depth in later chapters.\n",
        "\n",
        "**CBOW**\n",
        "\n",
        "The continuous-bag-of-words (CBOW) model uses context-target training pairs. Each pair will have all the context words as the first element and the target word as the second element. For example, if the context window is \"tom eats ***spicy*** crab salad\" (where ***spicy*** is the target word), the training pair would be\n",
        "\n",
        "((tom, eats, crab, salad), spicy)\n",
        "\n",
        "Since the context element for the CBOW model contains multiple words, we use the average context embedding vector when training our embedding model. So in the example above, the context embedding vector would be the average between the four embedding vectors for \"tom\", \"eats\", \"crab\", and \"salad\".\n",
        "\n",
        "**B. Skip-gram vs. CBOW**\n",
        "\n",
        "When it comes to choosing between the skip-gram and CBOW embedding models, there are a few things to consider. Since the skip-gram model creates a training pair for each context word, it requires much less actual data than the CBOW model. However, this also means that the CBOW model is faster to train.\n",
        "\n",
        "Furthermore, since the skip-gram model is creating multiple instances of training pairs for each target word, it can represent rarer words or phrases better than the CBOW model. On the other hand, the CBOW model provides more accurate embeddings for more common words.\n",
        "\n",
        "In this course, the training data is small, so we will create a skip-gram model. However, when creating your own personal embedding model for an NLP task, it's best to take into account all the factors when making a decision.\n",
        "\n",
        "#Time to Code!\n",
        "\n",
        "In this chapter, you'll be completing the ***create_target_context_pairs*** function, which creates target-context pairs for the skip-gram embedding model. Specifically, you'll be writing code in the portion of the function that says \"**CODE HERE**\".\n",
        "\n",
        "The target-context pairs we create will all be tuples containing ***target_word*** as the first element. The second element will be the context word ID at each context window index.\n",
        "\n",
        "At each index, ***j***, in\n",
        "\n",
        "***range(left_incl, right_excl)***\n",
        "\n",
        "where ***j != i***, append the tuple\n",
        "\n",
        "***(target_word, sequence[j])***\n",
        "\n",
        "to the end of ***pairs***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoxHyJ0V-R8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Skip-gram embedding model\n",
        "class EmbeddingModel(object):\n",
        "    # Model Initialization\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    def tokenize_text_corpus(self, texts):\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        return sequences\n",
        "\n",
        "    # Convert a list of text strings into word sequences\n",
        "    def get_target_and_context(self, sequence, target_index, window_size):\n",
        "        target_word = sequence[target_index]\n",
        "        half_window_size = window_size // 2\n",
        "        left_incl = max(0, target_index - half_window_size)\n",
        "        right_excl = min(len(sequence), target_index + half_window_size + 1)\n",
        "        return target_word, left_incl, right_excl\n",
        "    \n",
        "    # Create (target, context) pairs for a given window size\n",
        "    def create_target_context_pairs(self, texts, window_size):\n",
        "        pairs = []\n",
        "        sequences = self.tokenize_text_corpus(texts)      \n",
        "        for sequence in sequences:\n",
        "            for i in range(len(sequence)):\n",
        "                target_word, left_incl, right_excl = self.get_target_and_context(sequence, i, window_size)\n",
        "                for j in range(left_incl, right_excl):\n",
        "                    if j!=i:\n",
        "                        pairs.append((target_word, sequence[j]))\n",
        "\n",
        "                # CODE HERE\n",
        "        return pairs"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}