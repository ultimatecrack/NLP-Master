{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpVdoZ9YjyfrlfgX32dKfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultimatecrack/NLP-Master/blob/master/source1/nlp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G265gPcgHP1j",
        "colab_type": "text"
      },
      "source": [
        "#Vocabulary\n",
        "Become accustomed to the meaning of \"vocabulary\" for NLP tasks.\n",
        "\n",
        "Chapter Goals:\n",
        "Learn about the text corpus and vocabulary in NLP tasks\n",
        "Create a function that tokenizes a text corpus\n",
        "\n",
        "**A. Corpus vocabulary**\n",
        "\n",
        "In the context of NLP tasks, the text corpus refers to the set of texts used for the task. For example, if we were building a model to analyze news articles, our text corpus would be the entire set of articles or papers we used to train and evaluate the model.\n",
        "\n",
        "The set of unique words used in the text corpus is referred to as the vocabulary. When processing raw text for NLP, everything is done around the vocabulary.\n",
        "\n",
        "\n",
        "In addition to using the words of a text corpus as the vocabulary, you could also use a character-based vocabulary. This would consist of each unique character in the text corpus (e.g. each letter). In this course, we'll be focusing on word-based vocabularies, which are much more common than their character-based counterparts.\n",
        "\n",
        "**B. Tokenization**\n",
        "\n",
        "We can use the vocabulary to find the number of times each word appears in the corpus, figure out which words are the most common or uncommon, and filter each text document based on the words that appear in it. However, the most important part of the vocabulary is that it allows us to represent each piece of text by the specific words that appear in it.\n",
        "\n",
        "Rather than being represented as one long string, a piece of text can be represented as a vector/list of its vocabulary words. This process is known as tokenization, where each individual vocabulary word in a piece of text is a token.\n",
        "\n",
        "Below we show an example of tokenization on a text corpus.\n",
        "\n",
        "In the example above, the punctuation is filtered out of the text corpus. While it is normally standard to filter out punctuation, in some cases (e.g. generating long text) it may be necessary to keep punctuation in the vocabulary. It is a good idea to understand the NLP task you are going to perform before filtering out any data/piece of text.\n",
        "\n",
        "**C. Tokenizer object**\n",
        "\n",
        "Using TensorFlow, we can convert a text corpus into tokenized sequences using the Tokenizer object. The Tokenizer class is part of the tf.keras submodule, which is TensorFlow's implementation of Keras, a high-level API for machine learning.\n",
        "\n",
        "The Tokenizer object contains the functions fit_on_texts and texts_to_sequences, which are used to initialize the object with a text corpus and convert pieces of text into sequences of tokens, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZe67qTyGmKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68338f14-c6b7-4334-c29e-b0a21326d789"
      },
      "source": [
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "new_texts = ['bob ate pears', 'fred ate pears']\n",
        "print(tokenizer.texts_to_sequences(new_texts))\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 5], [6, 1, 5]]\n",
            "{'ate': 1, 'apples': 2, 'bob': 3, 'and': 4, 'pears': 5, 'fred': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfxkVz6UItjS",
        "colab_type": "text"
      },
      "source": [
        "The Tokenizer automatically converts each vocabulary word to an integer ID (IDs are given to words by descending frequency). This allows the tokenized sequences to be used in NLP algorithms (which work on vectors of numbers). In the above example, the texts_to_sequences function converts each vocabulary word in new_texts to its corresponding integer ID.\n",
        "\n",
        "**D. Tokenizer parameters**\n",
        "\n",
        "The Tokenizer object can be initialized with a number of optional parameters. By default, the Tokenizer filters out any punctuation and white space. You can specify custom filtering with the filters parameter. The parameter takes in a string, where each character in the string is filtered out.\n",
        "\n",
        "When a new text contains words not in the corpus vocabulary, those words are known as ***out-of-vocabulary (OOV)*** words. The texts_to_sequences automatically filters out all OOV words. However, if we want to specify each OOV word with a special vocabulary token (e.g. 'OOV'), we can initialize the Tokenizer with the oov_token parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQKlns4oHGHt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1f67b30e-53bd-414f-da6f-5ce87bfec77e"
      },
      "source": [
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "  oov_token='OOV')\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "print(tokenizer.texts_to_sequences(['bob ate bacon']))\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1]]\n",
            "{'OOV': 1, 'ate': 2, 'apples': 3, 'bob': 4, 'and': 5, 'pears': 6, 'fred': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTodfJFWIsiA",
        "colab_type": "text"
      },
      "source": [
        "The num_words parameter lets us specify the maximum number of vocabulary words to use. For example, if we set num_words=100 when initializing the Tokenizer, it will only use the 100 most frequent words in the vocabulary and filter out the remaining vocabulary words. This can be useful when the text corpus is large and you need to limit the vocabulary size to increase training speed or prevent overfitting on infrequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urXWgjT1JZLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dba5fa59-36cc-456b-ca11-701e521ae58f"
      },
      "source": [
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2)\n",
        "text_corpus = ['bob ate apples, and pears', 'fred ate apples!']\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "# the two most common words are 'ate' and 'apples'\n",
        "# the tokenizer will filter out all other words\n",
        "# for the sentence 'bob ate pears', only 'ate' will be kep\n",
        "# since 'ate' maps to an integer ID of 1, the only value \n",
        "# in the token sequence will be 1\n",
        "print(tokenizer.texts_to_sequences(['bob ate pears']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRiiZd1yJr82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a7d99a6-0543-45e2-f836-31e958b58efd"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ate': 1, 'apples': 2, 'bob': 3, 'and': 4, 'pears': 5, 'fred': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0s-uUzuKnni",
        "colab_type": "text"
      },
      "source": [
        "#Time to Code!\n",
        "The code for this section of the course involves building up an embedding model. Specifically, you will be building out the EmbeddingModel object. In this chapter, you'll be completing the ***tokenize_text_corpus*** function.\n",
        "\n",
        "You'll notice that in the model initialization, the ***Tokenizer*** object is already set, with its maximum vocabulary size set to ***vocab_size***. However, the ***Tokenizer*** object has not yet been initialized with a text corpus.\n",
        "\n",
        "In the ***tokenize_text_corpus*** function, we'll first initialize the Tokenizer with the text corpus, texts.\n",
        "\n",
        "Call ***self.tokenizer.fit_on_texts*** on texts.\n",
        "\n",
        "After initializing the Tokenizer with the text corpus, we can use it to convert the text corpus into tokenized sequences.\n",
        "\n",
        "Set sequences equal to ***self.tokenizer.texts_to_sequences*** applied to texts. Then return sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms0EtdTWKhIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Skip-gram embedding model\n",
        "class EmbeddingModel(object):\n",
        "    # Model Initialization\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    # Convert a list of text strings into word sequences\n",
        "    def tokenize_text_corpus(self, texts):\n",
        "        # CODE HERE\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        return sequences"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}