{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPac8uQI4mHhknsS+qKyMto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultimatecrack/NLP-Master/blob/master/source1/nlp5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7mAvYFfXg1k",
        "colab_type": "text"
      },
      "source": [
        "#Candidate Sampling\n",
        "Understand why candidate sampling is used for embedding training.\n",
        "\n",
        "Chapter Goals:\n",
        "  1.Learn about candidate sampling and why it is useful for embedding training\n",
        "\n",
        "**A. Large vocabularies**\n",
        "\n",
        "To obtain good word embeddings, it is usually necessary to train an embedding model on a large amount of text data. This means that the vocabulary size will likely be very large, often reaching tens of thousands of words. However, having a large vocabulary size can significantly slow down training.\n",
        "\n",
        "Training an embedding model is equivalent to multiclass classification, where the possible classes include every single vocabulary word. This means that we would need to calculate a softmax loss across every single vocabulary word during training, which can be incredibly time consuming for large vocabularies.\n",
        "\n",
        "In order to mitigate the costly full softmax operation, we apply something called candidate sampling. With candidate sampling, we choose a much smaller fraction of the possible classes (i.e. vocabulary words) for computing the loss. This speeds up training significantly while also maintaining performance if we use the proper candidate samplers and loss function (more on this in the next chapter).\n",
        "\n",
        "**B. Computing logits**\n",
        "\n",
        "When we calculate the loss for an embedding model, we need to first compute the model's logits. We do this by setting up trainable weights and bias terms, which will be variables created by the tf.get_variable function.\n",
        "\n",
        "Similar to the final layer of a multilayer perceptron, which computes the MLP's logits, the weights and bias variables for the embedding model also compute the logits, which are then converted into the loss based on the loss function.\n",
        "\n",
        "#Time to Code!\n",
        "\n",
        "In this chapter, you'll be completing the ***get_bias_weights*** function, which gets the bias and weights for calculating the embedding loss (next chapter).\n",
        "\n",
        "In order to calculate the loss for our candidate sampling algorithm, we need to create weight and bias variables. The weight variable will have shape ***[self.vocab_size, self.embedding_dim]***, while the bias variable will have shape ***[self.vocab_size]***.\n",
        "\n",
        "We'll initialize the values for both the weight and bias variables to all 0's.\n",
        "\n",
        "Set ***weights_initializer*** equal to ***tf.zeros*** applied with the weight variable's shape as the only argument.\n",
        "\n",
        "Set ***bias_initializer*** equal to ***tf.zeros*** applied with the bias variable's shape as the only argument.\n",
        "\n",
        "Next we'll create the weight and bias variables using these initializers and the tf.get_variable function.\n",
        "\n",
        "Set ***weights*** equal to ***tf.get_variable*** with ***'weights'*** as the required argument ***weights_initializer*** as the ***initializer*** keyword argument.\n",
        "\n",
        "Set ***bias*** equal to ***tf.get_variable*** with ***'bias'*** as the required argument ***bias_initializer*** as the ***initializer*** keyword argument.\n",
        "\n",
        "Return a tuple with ***weights*** as the first element and ***bias*** as the second element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVFlqKxlXd0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Skip-gram embedding model\n",
        "class EmbeddingModel(object):\n",
        "    # Model Initialization\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    # Get bias and weights for calculating loss\n",
        "    def get_bias_weights(self):\n",
        "        weights_initializer = tf.zeros([self.vocab_size, self.embedding_dim])\n",
        "        bias_initializer = tf.zeros([self.vocab_size])\n",
        "        weights = tf.get_variable('weights',\n",
        "        initializer=weights_initializer)\n",
        "        bias = tf.get_variable('bias',\n",
        "        initializer=bias_initializer)\n",
        "        return weights, bias"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}