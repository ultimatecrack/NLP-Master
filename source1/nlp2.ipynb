{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHWBEUdq9PYMl7gMV0mLj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ultimatecrack/NLP-Master/blob/master/source1/nlp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9FZNpqPVsSu",
        "colab_type": "text"
      },
      "source": [
        "#Embeddings\n",
        "Learn the basics of word embeddings and why they're used.\n",
        "\n",
        "Chapter Goals:\n",
        "  1. Learn about word embeddings and why they're used\n",
        "  2. Create a function that retrieves a target word and its context\n",
        "\n",
        "**A. Word representation**\n",
        "\n",
        "So far, the way we've represented vocabulary words is with unique integer IDs. However, these integer IDs don't give a sense of how different words may be related. For example, if a vocabulary gave the words \"milk\" and \"cereal\" the IDs 14 and 103, respectively, we would have no idea that these two words are often used within the same context.\n",
        "\n",
        "The solution to this problem is to convert each word into an embedding vector. An embedding vector is a higher-dimensional vector representation of a vocabulary word. Since vectors have a sense of distance (as they are just points in a higher-dimensional space), embedding vectors give us a word representation that captures relationships between words.\n",
        "\n",
        "Below is an example of word embedding vectors from the [TensorFlow Embedding Projector](https://projector.tensorflow.org/). Note that the words most similar to \"sheep\" have shorter pointwise distances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaDALBFWMCQ",
        "colab_type": "text"
      },
      "source": [
        "When creating embedding vectors for your vocabulary, something to consider is how large the vectors are (i.e. the number of dimensions). Larger vectors are able to capture more relational tendencies between words, and are therefore better if you have a large vocabulary size. However, they also use up more computational resources and are more likely to overfit on smaller vocabularies.\n",
        "\n",
        "A general rule of thumb is to set the number of dimensions in your embedding vectors equal to the 4th root of the vocabulary size. For example, if your vocabulary consists of 10000 words, the rule says your embedding vectors should have 10 dimensions. However, similar to the batch size or number of hidden layers in a neural network, you should try different embedding sizes to figure out which is best for the task at hand.\n",
        "\n",
        "**B. Target-context**\n",
        "\n",
        "The basis for embedding vectors comes from the concept of a target word and its context window. For each word of each tokenized sequence in the text corpus, we treat it as a target, and the words around it as the context window. This is how we define embedding vector relations; words that often appear near each other tend to be related.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXc-MFaYBae",
        "colab_type": "text"
      },
      "source": [
        "We refer to the size of the context window as the window size. Since we want the context to be symmetric on both sides of the target word, the window size should be an odd number. In the above example, the window size is 5 (two words to the left and right of the target word).\n",
        "\n",
        "Weâ€™ll be learning how to convert target words and context windows into data that can be used to train an embedding vector model in the next chapter.\n",
        "\n",
        "#Time to Code!\n",
        "In this chapter, you'll be completing the ***get_target_and_context*** function. This involves completing several helper functions.\n",
        "\n",
        "The first helper function to complete is ***get_target_and_size***. This returns a tuple containing the target word and half the window size.\n",
        "\n",
        "The target word is located at ***target_index*** of ***sequence***.\n",
        "\n",
        "Set target_word equal to the element located at target_index of the list sequence.\n",
        "\n",
        "Since the window's context is symmetric around the target word, the context will have length equal to half the window size (rounded down) on either side of the target word.\n",
        "\n",
        "Set ***half_window_size*** equal to ***window_size*** divided by ***2***, rounded down.\n",
        "\n",
        "Then return a tuple with ***target_word*** as the first element and ***half_window_size*** as the second element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBoll-jeVrAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_target_and_size(sequence, target_index, window_size):\n",
        "    # CODE HERE\n",
        "    target_word = sequence[target_index]\n",
        "    half_window_size = window_size//2\n",
        "    return target_word, half_window_size\n",
        "\n",
        "# Skip-gram embedding model\n",
        "class EmbeddingModel(object):\n",
        "    # Model Initialization\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    # Convert a list of text strings into word sequences\n",
        "    def get_target_and_context(self, sequence, target_index, window_size):\n",
        "        target_word, half_window_size = get_target_and_size(\n",
        "            sequence, target_index, window_size\n",
        "        )"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsI0vfuyafXq",
        "colab_type": "text"
      },
      "source": [
        "Next, you'll complete the helper function ***get_window_indices***. This returns a tuple containing the left (inclusive) and right (exclusive) indices of the window.\n",
        "\n",
        "To match Pythonic ***range*** indexing, we set the window's start index as inclusive and the stop index as exclusive. We also need to make sure the window does not extend out of the indexable range of ***sequence***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdxslmuyahi-",
        "colab_type": "text"
      },
      "source": [
        "Set ***left_incl*** equal to the maximum between ***0*** and ***target_index - half_window_size***.\n",
        "\n",
        "Set ***right_excl*** equal to the minimum between the length of ***sequence*** and ***target_index + half_window_size + 1***.\n",
        "\n",
        "Then return a tuple with ***left_incl*** as the first element and ***right_excl*** as the second element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhFVf5H5aYYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_window_indices(sequence, target_index, half_window_size):\n",
        "    # CODE HERE\n",
        "    left_incl = max(0, target_index - half_window_size)\n",
        "    right_excl = min(len(sequence), target_index + half_window_size + 1)\n",
        "    return left_incl, right_excl\n",
        "    pass\n",
        "\n",
        "# Skip-gram embedding model\n",
        "class EmbeddingModel(object):\n",
        "    # Model Initialization\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.vocab_size)\n",
        "\n",
        "    # Convert a list of text strings into word sequences\n",
        "    def get_target_and_context(self, sequence, target_index, window_size):\n",
        "        target_word, half_window_size = get_target_and_size(\n",
        "            sequence, target_index, window_size\n",
        "        )\n",
        "        left_incl, right_excl = get_window_indices(\n",
        "            sequence, target_index, half_window_size)\n",
        "        return target_word, left_incl, right_excl"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}